{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "inpath = '/MFMDatasets/MFM_bopf/src/multi_predict/'\n",
    "#inpath = '/slurm_storage/mbopf/projects/MFM_bopf/src/multi_predict/'\n",
    "outpath = '/MFMDatasets/MFM_bopf/src/multi_predict/results/'\n",
    "out_flag = True\n",
    "#out_flag = False\n",
    "\n",
    "Cutoff_stat = \"Fb2_2\"\n",
    "SORT_STAT = \"Fb2_2\"\n",
    "#Cutoff_stat = \"MCC\"\n",
    "#SORT_STAT = \"MCC\"\n",
    "#Cutoff_stat = \"PR_AUC\"\n",
    "#SORT_STAT = \"PR_AUC\"\n",
    "#Cutoff_stat = \"ROC_AUC\"\n",
    "#SORT_STAT = \"ROC_AUC\"\n",
    "\n",
    "# Cut off value for given stat.: NOTE: latter parts of notebook won't work due to missing indexes\n",
    "CUTOFF = 0.0\n",
    "\n",
    "\n",
    "#alg_cols = ['CLF_time(min)','target','under_alg']\n",
    "alg_cols = ['CLF_time(min)','target','period','feats','under_alg']\n",
    "#alg_cols = ['CLF_time(min)','target','under_alg','samp_strat']\n",
    "#stat_cols = ['TP','FN','FP','TN','precision_2','recall_2','Spec_2',\n",
    "stat_cols = ['TP','FN','FP','TN','NTP','NFN','NFP','NTN','precision_2','recall_2','Spec_2',\n",
    "            'ROC_AUC','PR_AUC','MCC','Fb1_2','Fb2_2','Fb05_2',\n",
    "            'Gmean_2','Gmean_ma','Max Gmean','Max Thresh'\n",
    "            ]\n",
    "\n",
    "#period = \"-PI\"\n",
    "period = \"-Pre\"\n",
    "\n",
    "#alg = \"LR\"\n",
    "#spec_cols = ['p_C','p_solver']\n",
    "\n",
    "#alg = \"NB\"\n",
    "#spec_cols = ['p_var_smoothing']\n",
    "\n",
    "#alg = \"SVC\"\n",
    "#spec_cols = ['p_C','p_kernel','p_degree']\n",
    "\n",
    "#alg = \"MLP\"\n",
    "#spec_cols = ['p_alpha','p_hidden_layer_sizes']\n",
    "\n",
    "alg = \"RF\"\n",
    "spec_cols = ['p_n_estimators','p_max_depth','p_min_samples_leaf','p_min_samples_split']\n",
    "\n",
    "#alg = \"GB\"\n",
    "#spec_cols = ['p_n_estimators','p_max_depth','p_subsample','p_learning_rate']\n",
    "\n",
    "alg_cols.extend(spec_cols)\n",
    "alg_cols.extend(stat_cols)\n",
    "\n",
    "os.chdir(inpath + alg + '/output')\n",
    "#os.chdir(inpath + alg + '/2021_02_Cramer/output')\n",
    "#os.chdir(inpath + alg + '/Oct2020/output')\n",
    "#os.chdir(inpath + alg + '/Nov2020/output')\n",
    "\n",
    "#dataset = \"transfus_yes\"\n",
    "#dataset = \"Hysterectomy\"\n",
    "dataset = \"*\"\n",
    "#dataset = \"trans*\"\n",
    "ext = '.csv'\n",
    "glob_string = dataset + period + \"-*\" + alg + \"-*\"\n",
    "files = glob.glob(glob_string + ext)\n",
    "#files = glob.glob(\"*-\" + alg + \"-*\" + ext)\n",
    "#files = glob.glob(\"transfus_yes*-\" + alg + \"-*\" + ext)\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "#os.chdir('/slurm_storage/mbopf/projects/MFM_bopf/src/multi_predict/MLP/output')\n",
    "#os.chdir('/slurm_storage/mbopf/projects/MFM_bopf/src/multi_predict/SVC/output')\n",
    "#os.chdir('/MFMDatasets/MFM_bopf/src/multi_predict/NB/output')\n",
    "#files = glob.glob(\"*-NB-*.csv\")\n",
    "#files = glob.glob(\"*-MLP-*.csv\")\n",
    "#files = glob.glob(\"*-SVC-*.csv\")\n",
    "#files = glob.glob(\"transfus_yes*-MLP-*.csv\")\n",
    "#files = glob.glob(\"transfus_yes*-MLP-*]_0.*_500*.csv\")\n",
    "#files = glob.glob(\"transfus_hyster*-MLP-*.csv\")\n",
    "#files = glob.glob(\"transfus_yes*-MLP-*.csv\")\n",
    "#files = glob.glob(\"transfus_yes*-MLP-*.csv\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dups(file):\n",
    "    base = glob.escape(file[:-4])\n",
    "    print(f'base = {base}')\n",
    "    dup_files = glob.glob(base + '*')\n",
    "    print(dup_files)\n",
    "    for dup_file in dup_files:\n",
    "        print(f'Removing duplicate: {dup_file}')\n",
    "        os.remove(dup_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(files, reverse=True)\n",
    "for i in range(len(files)-1):\n",
    "    if files[i][:-20] == files[i+1][:-20]:\n",
    "        print(f'DUP:{files[i]}')\n",
    "        #remove_dups(files[i])\n",
    "\n",
    "files = glob.glob(glob_string + ext)\n",
    "files = sorted(files, reverse=True)\n",
    "print(f'len(files) = {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_f = \"\"\n",
    "for idx, f in enumerate(files):\n",
    "    print(f'idx={idx}; file={f}')\n",
    "    if prev_f[:-20] == f[:-20]:   #if the filenames only differ by timestamp\n",
    "        print(f'DUP:{prev_f}')\n",
    "        continue   #skip \"duplicate\"\n",
    "        \n",
    "    if results_df.empty:\n",
    "        results_df = pd.read_csv(f, header=None, index_col=0, names=[idx])\n",
    "    else:\n",
    "        this_df = pd.read_csv(f, header=None, index_col=0, names=[idx])\n",
    "        results_df = results_df.merge(this_df, left_index=True, right_index=True, how=\"right\")\n",
    "    prev_f = f\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_T = results_df.T\n",
    "#results_T.sort_values(by=['ROC_AUC'], ascending=False, inplace=True)\n",
    "#results_T.sort_values(by=['PR_AUC'], ascending=False, inplace=True)\n",
    "#results_T.sort_values(by=['MCC'], ascending=False, inplace=True)\n",
    "results_T[SORT_STAT] = pd.to_numeric(results_T[SORT_STAT])  # convert to float for sorting\n",
    "results_T.sort_values(by=[SORT_STAT], ascending=False, inplace=True)\n",
    "results_T['TP'] = results_T['TP'].astype(int)\n",
    "results_T['FN'] = results_T['FN'].astype(int)\n",
    "results_T['FP'] = results_T['FP'].astype(int)\n",
    "results_T['TN'] = results_T['TN'].astype(int)\n",
    "denom = results_T['TP'] + results_T['FN'] + results_T['FP'] + results_T['TN']\n",
    "results_T.loc[:,'NTP'] = np.around(1000 * results_T['TP'] / denom, 0).astype(int)\n",
    "results_T.loc[:,'NFN'] = np.around(1000 * results_T['FN'] / denom, 0).astype(int)\n",
    "results_T.loc[:,'NFP'] = np.around(1000 * results_T['FP'] / denom, 0).astype(int)\n",
    "results_T.loc[:,'NTN'] = np.around(1000 * results_T['TN'] / denom, 0).astype(int)\n",
    "\n",
    "small = results_T[alg_cols]\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if out_flag:\n",
    "    if dataset == '*':\n",
    "        #small.to_csv(outpath + alg + '-ALL-' + SORT_STAT + '-Results.csv')\n",
    "        small.to_csv(outpath + alg + '-' + SORT_STAT + '-Results.csv', float_format='%0.4f')\n",
    "    else:\n",
    "        small.to_csv(outpath + alg + \"-\" + dataset + '-' + SORT_STAT + '-Results.csv', float_format='%0.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already in the correct directory from above: os.chdir(inpath + alg + '/output')\n",
    "ext = '.out'\n",
    "#files = glob.glob(\"*-\" + alg + \"-*\" + ext)\n",
    "#files = glob.glob(\"Hyster\" + glob_prefix + ext)\n",
    "files = glob.glob(glob_string + ext)\n",
    "print(f'len(files) = {len(files)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = sorted(files, reverse=True)\n",
    "for i in range(len(files)-1):\n",
    "    #print(f'i={i}; file={files[i]}')\n",
    "    if files[i][:-20] == files[i+1][:-20]:\n",
    "        print(f'{files[i]}')\n",
    "print(f'len(files) = {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tokenize import tokenize\n",
    "\n",
    "high_coeff_files = set()\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "prev_f = \"\"\n",
    "coeffs_df = pd.DataFrame()\n",
    "coeffs_rank_df = pd.DataFrame()\n",
    "for idx, f in enumerate(files):\n",
    "    print(f'idx={idx}; file={f}')\n",
    "    if prev_f[:-20] == f[:-20]:   #if the filenames only differ by timestamp\n",
    "        print(f'DUP:{prev_f}')\n",
    "        continue   #skip \"duplicate\"\n",
    " \n",
    "    with open(f, 'r') as file:\n",
    "        found_coef = False\n",
    "        cutoff = False\n",
    "        coef_series = pd.Series()\n",
    "        ind_vars = []\n",
    "        coef_vals = []\n",
    "        for line in file:\n",
    "            # Cutoff stat\n",
    "            if Cutoff_stat in line:\n",
    "                cut_stat = line.split()[2]   \n",
    "                if float(cut_stat) < CUTOFF:\n",
    "                    cutoff = True\n",
    "                    print(f'cut_stat = {cut_stat}; idx={idx}')\n",
    "                    break    # Stop processing this file; poor performance\n",
    "            \n",
    "            if \"coeffs =\" in line:\n",
    "                found_coef = True\n",
    "                #print(f\"found_coef: idx={idx}; file={f}\")\n",
    "                continue\n",
    "            \n",
    "            if found_coef:\n",
    "                var_coef = line.split()\n",
    "                if var_coef[0] != 'dtype:':\n",
    "                    ind_vars.append(var_coef[0])\n",
    "                    coef_vals.append(float(var_coef[1]))\n",
    "                    if float(var_coef[1]) > 100:\n",
    "                        high_coeff_files.add(f)\n",
    "                \n",
    "        if not cutoff and found_coef:\n",
    "            if coeffs_df.empty:\n",
    "                coeffs_df = pd.DataFrame(data=coef_vals, index=ind_vars, columns=[idx])\n",
    "                coeffs_rank_df = pd.DataFrame(data=coeffs_df[idx].rank(ascending=False).astype(int), columns=[idx])\n",
    "            else:\n",
    "                this_df = pd.DataFrame(data=coef_vals, index=ind_vars, columns=[idx])\n",
    "                this_rank_df = pd.DataFrame(data=this_df[idx].rank(ascending=False).astype(int), columns=[idx])\n",
    "                coeffs_df = coeffs_df.merge(this_df, left_index=True, right_index=True, how=\"left\")\n",
    "                coeffs_rank_df = coeffs_rank_df.merge(this_rank_df, left_index=True, right_index=True, how=\"left\")\n",
    "\n",
    "    prev_f = f\n",
    "    \n",
    "coeffs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if out_flag:\n",
    "#    if dataset == '*':\n",
    "#        coeffs_df.to_csv(outpath + alg + '-ALL-coeffs.csv')\n",
    "#    elif dataset == 'trans*':\n",
    "#        coeffs_df.to_csv(outpath + alg + '-TRANS-coeffs.csv')\n",
    "#    else:\n",
    "#        coeffs_df.to_csv(outpath + alg + \"-\" + dataset + '-coeffs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs_rank_df['mean_rank'] = coeffs_rank_df.apply(np.mean, axis=1).rank().astype(int)\n",
    "coeffs_rank_df = coeffs_rank_df.astype('Int64')\n",
    "coeffs_rank_df.sort_values('mean_rank', inplace=True)\n",
    "print(f'Count: {coeffs_rank_df.shape}')\n",
    "len(coeffs_rank_df)\n",
    "coeffs_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if out_flag:\n",
    "#    if dataset == '*':\n",
    "#        coeffs_rank_df.to_csv(outpath + alg + '-' + SORT_STAT + '-coef_rank.csv')\n",
    "#    elif dataset == 'trans*':\n",
    "#        coeffs_rank_df.to_csv(outpath + alg + '-TRANS-coef_rank.csv')\n",
    "#    else:\n",
    "#        coeffs_rank_df.to_csv(outpath + alg + \"-\" + dataset + '-coef_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs_rank_inv = coeffs_rank_df.T\n",
    "combo = pd.concat([small, coeffs_rank_inv], axis=1, join='inner')\n",
    "combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if out_flag:\n",
    "    if dataset == '*':\n",
    "        combo.to_csv(outpath + alg + '-' + SORT_STAT + '-combo.csv')\n",
    "    else:\n",
    "        combo.to_csv(outpath + alg + \"-\" + dataset + '-combo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coeffs_df_inv = coeffs_df.T\n",
    "#combo2 = pd.concat([combo, coeffs_df_inv['abruptio9']], axis=1, join='inner')\n",
    "combo2 = pd.concat([combo, coeffs_df_inv], axis=1, join='inner')\n",
    "combo2\n",
    "#combo2.sort_values('abruptio9', inplace=True) # Can't do this because there are two \"abruptio9\" cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if out_flag:\n",
    "    if dataset == '*':\n",
    "        combo2.to_csv(outpath + alg + '-' + SORT_STAT + '-combo2.csv')\n",
    "    else:\n",
    "        combo2.to_csv(outpath + alg + \"-\" + dataset + '-combo2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_index = combo.iloc[0].name\n",
    "print(f'top_index = {top_index}')\n",
    "#top_index = 50\n",
    "files[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small.loc[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = glob.escape(files[top_index][:-20])\n",
    "print(base)\n",
    "#base = glob.escape(base)\n",
    "#print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = inpath + alg + '/output/'\n",
    "ft_glob = base + \"*\" + '_fpr_tpr.dat'\n",
    "pr_glob = base + \"*\" + '_pr.dat'\n",
    "pred_glob = base + \"*\" + '_pred.dat'\n",
    "prob_glob = base + \"*\" + '_probs.dat'\n",
    "print(glob.glob(ft_glob))\n",
    "print(glob.glob(pr_glob))\n",
    "print(glob.glob(pred_glob))\n",
    "print(glob.glob(prob_glob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = np.loadtxt(glob.glob(ft_glob)[0], delimiter=',')\n",
    "precision, recall, pr_thresh = np.loadtxt(glob.glob(pr_glob)[0], delimiter=',')\n",
    "y_test, y_pred = np.loadtxt(glob.glob(pred_glob)[0], dtype=int, delimiter=',')\n",
    "probs = np.loadtxt(glob.glob(prob_glob)[0], delimiter=',')\n",
    "prob1 = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, matthews_corrcoef, auc, roc_auc_score, precision_score\n",
    "from imblearn.metrics import geometric_mean_score, sensitivity_specificity_support\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "roc_auc_s_prob = roc_auc_score(y_test, prob1)\n",
    "print(f'ROC_AUC = {roc_auc_s_prob}')\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f' PR_AUC = {pr_auc}')\n",
    "#prec = precision_score(y_test, y_pred, pos_label=2)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "print(f'   PREC = {prec}')\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(f'    MCC = {mcc}')\n",
    "# Note: pos_label does not appear to have any effect\n",
    "gmean_none = geometric_mean_score(y_test, y_pred, average=None)\n",
    "print(f'gmean_none    = {gmean_none}')\n",
    "gmean_macro = geometric_mean_score(y_test, y_pred, average='macro')\n",
    "print(f'gmean_macro   = {gmean_macro}')\n",
    "sens, spec, ss_supp = sensitivity_specificity_support(y_test, y_pred, average=None)\n",
    "print(f'sens   = {sens}')\n",
    "print(f'spec   = {spec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_test.shape = {y_test.shape}')\n",
    "print(f'prob1.shape = {prob1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc vs roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, auc\n",
    "roc_auc_s_prob = roc_auc_score(y_test, prob1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f'roc_auc_s_prob = {roc_auc_s_prob}')\n",
    "print(f'roc_auc(auc) = {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('TPRate')\n",
    "plt.xlabel('FPRate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the no skill line as the proportion of the positive class\n",
    "no_skill = len(y_test[y_test==2]) / len(y_test)\n",
    "# plot the no skill precision-recall curve\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle= '--', label= f'No Skill: {no_skill:.3f}')\n",
    "\n",
    "pr_auc = auc(recall, precision)\n",
    "plt.title('PR Curve')\n",
    "plt.plot(recall, precision, 'g', label = 'AUC = %0.3f' % pr_auc)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "print(f'PR_AUC = {pr_auc}')\n",
    "print(f'AP     = {average_precision_score(y_test, prob1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Probabilities')\n",
    "plt.hist(prob1, bins=50)\n",
    "plt.ylabel('Counts')\n",
    "plt.xlabel('Minority probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmeans = np.sqrt(tpr * (1-fpr))\n",
    "print(gmeans.shape)\n",
    "plt.plot(gmeans)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the index of the largest g-mean\n",
    "ix = np.argmax(gmeans)\n",
    "print(f'max_ix = {ix}')\n",
    "print( ' Best Threshold=%f, G-mean=%.3f ' % (thresholds[ix], gmeans[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve for the model\n",
    "plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "plt.plot(fpr, tpr, 'r', label='Logistic')\n",
    "plt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ix = {ix}; fpr[ix] = {fpr[ix]}; tpr[ix]={tpr[ix]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "ix = np.argmax(fscore)\n",
    "print( ' Best Threshold=%f, F-measure=%.3f ' % (pr_thresh[ix], fscore[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "#plt.plot([0,1], [no_skill,no_skill], linestyle= '--' , label= 'No Skill' )\n",
    "plt.plot(recall, precision, label= 'PR curve' )\n",
    "plt.scatter(recall[ix], precision[ix], marker= 'o' , color= 'black' , label= 'Best' )\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred, normalize='true'))\n",
    "print(confusion_matrix(y_test, y_pred, normalize='pred'))\n",
    "print(confusion_matrix(y_test, y_pred, normalize='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Cramer to \"top index\" coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'top_index = {top_index}')\n",
    "top_coeffs = pd.DataFrame(coeffs_df[top_index])\n",
    "#top_coeffs = pd.DataFrame(coeffs_df[top_index], columns=['coeff'])\n",
    "top_coeffs.rename(columns={top_index:'coeff'}, inplace=True)\n",
    "top_coeffs.index.name = 'Variable'\n",
    "top_coeffs.sort_values(by='coeff', ascending=False, inplace=True)\n",
    "top_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_coeffs['coeff_rank'] = top_coeffs['coeff'].rank(ascending=False).astype(int)\n",
    "top_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_index)\n",
    "small.loc[top_index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_target = small.loc[top_index]['target']\n",
    "top_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramfile =  cramer_df = '/MFMDatasets/MFM_bopf/data/csl/Cramer-corr-' + top_target + '.csv'\n",
    "print(cramfile)\n",
    "cramer_df = pd.read_csv(cramfile, header=None, index_col=0, names = ['Variable', 'Cramer_corr'])\n",
    "cramer_df['Cramer_rank'] = cramer_df['Cramer_corr'].rank(ascending=False).astype(int)\n",
    "cramer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_df = pd.merge(top_coeffs, cramer_df, how='inner', on='Variable')\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# @author: Ritesh Agrawal\n",
    "# @Date: 13 Feb 2013\n",
    "# @Description: This is an implementation of rank biased overlap score \n",
    "# (Refererence: http://www.umiacs.umd.edu/~wew/papers/wmz10_tois.pdf). \n",
    "# This is a modified implementation of  https://github.com/maslinych/linis-scripts/blob/master/rbo_calc.py\n",
    "# It is a linear implementation of the RBO and assumes there are no\n",
    "# duplicates and doesn't handle for ties. \n",
    "#\n",
    "\n",
    "def score(l1, l2, p = 0.98):\n",
    "    \"\"\"\n",
    "        Calculates Ranked Biased Overlap (RBO) score. \n",
    "        l1 -- Ranked List 1\n",
    "        l2 -- Ranked List 2\n",
    "    \"\"\"\n",
    "    if l1 == None: l1 = []\n",
    "    if l2 == None: l2 = []\n",
    "    \n",
    "    sl,ll = sorted([(len(l1), l1),(len(l2),l2)])\n",
    "    s, S = sl\n",
    "    l, L = ll\n",
    "    if s == 0: return 0\n",
    "\n",
    "    # Calculate the overlaps at ranks 1 through l \n",
    "    # (the longer of the two lists)\n",
    "    ss = set([]) # contains elements from the smaller list till depth i\n",
    "    ls = set([]) # contains elements from the longer list till depth i\n",
    "    x_d = {0: 0}\n",
    "    sum1 = 0.0\n",
    "    for i in range(l):\n",
    "        x = L[i]\n",
    "        y = S[i] if i < s else None\n",
    "        d = i + 1\n",
    "        \n",
    "        # if two elements are same then \n",
    "        # we don't need to add to either of the set\n",
    "        if x == y: \n",
    "            x_d[d] = x_d[d-1] + 1.0\n",
    "        # else add items to respective list\n",
    "        # and calculate overlap\n",
    "        else: \n",
    "            ls.add(x) \n",
    "            if y != None: ss.add(y)\n",
    "            x_d[d] = x_d[d-1] + (1.0 if x in ss else 0.0) + (1.0 if y in ls else 0.0)     \n",
    "        #calculate average overlap\n",
    "        sum1 += x_d[d]/d * pow(p, d)\n",
    "        \n",
    "    sum2 = 0.0\n",
    "    for i in range(l-s):\n",
    "        d = s+i+1\n",
    "        sum2 += x_d[d]*(d-s)/(d*s)*pow(p,d)\n",
    "\n",
    "    sum3 = ((x_d[l]-x_d[s])/l+x_d[s]/s)*pow(p,l)\n",
    "\n",
    "    # Equation 32\n",
    "    rbo_ext = (1-p)/p*(sum1+sum2)+sum3\n",
    "    return rbo_ext\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, mannwhitneyu\n",
    "# Calculate Rank-Based Overlap score\n",
    "rbo = score(compare_df['Cramer_rank'].tolist(), compare_df['coeff_rank'].tolist())\n",
    "print(f'rbo = {rbo}')\n",
    "ken_tau = kendalltau(compare_df['Cramer_rank'].tolist(), compare_df['coeff_rank'].tolist())\n",
    "print(f'ken_tau = {ken_tau}')\n",
    "mann_whit = mannwhitneyu(compare_df['Cramer_rank'].tolist(), compare_df['coeff_rank'].tolist())\n",
    "print(f'mann_whit = {mann_whit}')\n",
    "print(f'CLF_time(min) = {combo.loc[top_index][\"CLF_time(min)\"]}')\n",
    "print(f'{SORT_STAT} = {combo.loc[top_index][SORT_STAT]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = outpath + '/cramer_compare/' + alg + '-' + SORT_STAT + '-CramerCoeffComp.csv'\n",
    "with open(file, 'w', newline='') as outfile:\n",
    "    print(f'Algorithm, {alg}', file=outfile)\n",
    "    print(f'Target, {top_target}', file=outfile)\n",
    "    print(f'RBO, {rbo:.3f}', file=outfile)\n",
    "    print(f'Kendall_tau, {ken_tau.correlation:.3f}', file=outfile)\n",
    "    print(f'CLF_time(min) = {combo.loc[top_index][\"CLF_time(min)\"]}', file=outfile)\n",
    "    print(f'{SORT_STAT} = {combo.loc[top_index][SORT_STAT]}\\n', file=outfile)\n",
    "#    print(f'MannWhitneyU, {mann_whit.statistic:.3f}\\n', file=outfile)\n",
    "    \n",
    "if out_flag:\n",
    "    compare_df.to_csv(file, mode='a', float_format='%0.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "a2 = [25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1]\n",
    "a3 = [1,3,2,4,6,5]\n",
    "a4 = [5,3,1,4,6,2]\n",
    "a5 = [24,22,20,18,7,6,4,3,2,1]\n",
    "print(f'rbo(a1, a1) = {score(a1,a1)}')\n",
    "print(f'rbo(a1, a2) = {score(a1,a2)}')\n",
    "print(f'rbo(a1, a3) = {score(a1,a3)}')\n",
    "print(f'rbo(a1, a4) = {score(a1,a4)}')\n",
    "print(f'rbo(a1, a5) = {score(a1,a5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau\n",
    "print(f'ken_tau(a1, a1) = {kendalltau(a1,a1)}')\n",
    "print(f'ken_tau(a1, a2) = {kendalltau(a1,a2)}')\n",
    "print(f'ken_tau(a3, a4) = {kendalltau(a3,a4)}')\n",
    "#print(f'ken_tau(a1, a4) = {kendalltau(a1,a4)}')\n",
    "#print(f'ken_tau(a1, a5) = {kendalltau(a1,a5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'mann_whit(a1, a1) = {mannwhitneyu(a1,a1)}')\n",
    "print(f'mann_whit(a1, a2) = {mannwhitneyu(a1,a2)}')\n",
    "print(f'mann_whit(a3, a4) = {mannwhitneyu(a3,a4)}')\n",
    "#print(f'ken_tau(a1, a4) = {kendalltau(a1,a4)}')\n",
    "#print(f'ken_tau(a1, a5) = {kendalltau(a1,a5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
