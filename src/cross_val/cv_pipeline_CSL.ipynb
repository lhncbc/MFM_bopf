{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, recall_score, \\\n",
    "                            classification_report, roc_auc_score, precision_score, \\\n",
    "                            f1_score, matthews_corrcoef, average_precision_score, \\\n",
    "                            precision_recall_curve\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../data/csl/CSL_tl_PI.csv', index_col=0)  # MCC = 0.237 / ROC_AUC = 0.748\n",
    "df = pd.read_csv('../../data/csl/CSL_tl_PI_Freq.csv', index_col=0)  # MCC = 0.225 / ROC_AUC = 0.732\n",
    "X = df.drop('trans_loss', axis=1, inplace=False)\n",
    "y = df['trans_loss'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185413, 193)\n",
      "(185413,)\n",
      "Counter({0: 175069, 1: 10344})\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 175 ms, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(solver='liblinear', class_weight='balanced', C=0.1) #\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29992 13776]\n",
      " [  571  2015]]\n",
      "Accuracy = 0.6904905725503732\n",
      "Balanced Accuracy = 0.7322225831682572\n",
      "Recall = 0.7791956689868523\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81     43768\n",
      "           1       0.13      0.78      0.22      2586\n",
      "\n",
      "    accuracy                           0.69     46354\n",
      "   macro avg       0.55      0.73      0.51     46354\n",
      "weighted avg       0.93      0.69      0.77     46354\n",
      "\n",
      "ROC_AUC = 0.7322225831682572\n",
      "MCC = 0.2249180196722494\n",
      "Average precision/PR_AUC: 0.18\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')\n",
    "print(f'Recall = {recall_score(y_test, y_pred)}')\n",
    "print(f'\\nClassification Report:\\n {classification_report(y_test, y_pred)}')\n",
    "print(f'ROC_AUC = {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'MCC = {matthews_corrcoef(y_test, y_pred)}')\n",
    "probs = lr.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs, pos_label=1)\n",
    "y_score = lr.decision_function(X_test)\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "print('Average precision/PR_AUC: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF with Random Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 131301, 1: 65650})\n",
      "(196951, 193)\n"
     ]
    }
   ],
   "source": [
    "# define oversampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=0.5)\n",
    "# fit and apply the transform\n",
    "X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "# summarize class distribution\n",
    "print(Counter(y_over))\n",
    "print(X_over.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 604 ms, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=40)\n",
    "#clf = LogisticRegression(solver='liblinear', C=0.1) # \n",
    "clf.fit(X_over, y_over)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43420   348]\n",
      " [ 2479   107]]\n",
      "[[0.93670449 0.00750744]\n",
      " [0.05347974 0.00230832]]\n",
      "Accuracy = 0.9390128144280968\n",
      "Balanced Accuracy = 0.5167128145125186\n",
      "Recall = 0.04137664346481052\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     43768\n",
      "           1       0.24      0.04      0.07      2586\n",
      "\n",
      "    accuracy                           0.94     46354\n",
      "   macro avg       0.59      0.52      0.52     46354\n",
      "weighted avg       0.91      0.94      0.92     46354\n",
      "\n",
      "ROC_AUC = 0.5167128145125186\n",
      "MCC = 0.07781528095589256\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred, normalize='all'))\n",
    "print(f'Accuracy = {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Balanced Accuracy = {balanced_accuracy_score(y_test, y_pred)}')\n",
    "print(f'Recall = {recall_score(y_test, y_pred)}')\n",
    "print(f'\\nClassification Report:\\n {classification_report(y_test, y_pred)}')\n",
    "print(f'ROC_AUC = {roc_auc_score(y_test, y_pred)}')\n",
    "print(f'MCC = {matthews_corrcoef(y_test, y_pred)}')\n",
    "probs = clf.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, probs, pos_label=1)\n",
    "#y_score = clf.decision_function(X_test)\n",
    "#average_precision = average_precision_score(y_test, y_score)\n",
    "#print('Average precision/PR_AUC: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10517026 0.09073182 0.09853585 0.10502665 0.1027909 ]\n",
      "MCC: 0.100\n",
      "CPU times: user 176 ms, sys: 460 ms, total: 636 ms\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.under_sampling import RandomUnderSampler # MWB\n",
    "# define pipeline\n",
    "steps = [('over', RandomOverSampler()), ('model', DecisionTreeClassifier())]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# evaluate pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "def my_mcc(y_true,y_pred):\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    return mcc\n",
    "\n",
    "my_scorer = make_scorer(my_mcc, greater_is_better=True)\n",
    "\n",
    "scores = cross_val_score(pipeline, X, y, scoring=my_scorer, cv=cv, n_jobs=-1)\n",
    "print(scores)\n",
    "score = np.mean(scores)\n",
    "print('MCC: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25435809 0.25374443 0.25316007 0.24969889 0.24902289]\n",
      "MCC: 0.252\n",
      "CPU times: user 133 ms, sys: 129 ms, total: 262 ms\n",
      "Wall time: 7.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_score\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=40)\n",
    "over = RandomOverSampler(sampling_strategy=0.07)\n",
    "under = RandomUnderSampler(sampling_strategy=1.0)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "# define pipeline\n",
    "#pipeline = Pipeline(steps=[('o', over), ('u', under), ('m', model)])\n",
    "pipeline = Pipeline(steps=[('u', under), ('m', model)])\n",
    "scores = cross_val_score(pipeline, X, y, scoring=my_scorer, cv=cv, n_jobs=-1)\n",
    "print(scores)\n",
    "score = np.mean(scores)\n",
    "print('MCC: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([1.44662499, 1.37447858]), 'score_time': array([1.18791246, 1.1572578 ]), 'test_prec': array([0.13350919, 0.13379393]), 'test_mcc': array([0.25389369, 0.25439932])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_score\n",
    "#model = RandomForestClassifier(n_estimators=100, max_depth=40)\n",
    "model = GradientBoostingClassifier(n_estimators=100, max_depth=11, \n",
    "                                   min_samples_split=900, min_samples_leaf=50,\n",
    "                                   subsample=0.85, learning_rate=0.1, max_features=12)\n",
    "over = RandomOverSampler(sampling_strategy=0.07)\n",
    "under = RandomUnderSampler(sampling_strategy=1.0)\n",
    "cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=1, random_state=1)\n",
    "\n",
    "scoring = {'prec':'precision', 'mcc': my_scorer}\n",
    "# define pipeline\n",
    "#pipeline = Pipeline(steps=[('o', over), ('u', under), ('m', model)])\n",
    "pipeline = Pipeline(steps=[('u', under), ('m', model)])\n",
    "scores = cross_validate(pipeline, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
